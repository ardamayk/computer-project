import rclpy
import numpy as np
import threading
import time
import os
from datetime import datetime

from env import RobotEnv
from model_utils import Actor, Critic
from td3_agent import TD3
from replay_buffer import ReplayBuffer
from rclpy.executors import MultiThreadedExecutor
from geometry_msgs.msg import PointStamped
from visualization_msgs.msg import Marker
from visualization import RewardVisualizer
from model_io import load_checkpoint, save_checkpoint, list_available_models

def sample_target_ur5e(base_position=np.array([0,0,0]), max_reach=0.8, min_reach=0.1):
    """
    min_reach ve max_reach arasÄ±nda, homojen rastgele bir nokta Ã¶rnekler.
    Robotun eriÅŸim yarÄ±Ã§apÄ± 0.8 m (800 mm) olarak alÄ±nÄ±r.
    """
    max_reach = min(max_reach, 0.8)
    min_reach = max(0.0, min_reach)
    while True:
        u = np.random.uniform(0, 1)
        v = np.random.uniform(0, 1)
        w = np.random.uniform(0, 1)
        theta = 2 * np.pi * u
        phi = np.arccos(2 * v - 1)
        # r'yi min_reach ile max_reach arasÄ±nda homojen daÄŸÄ±tmak iÃ§in:
        r = ((max_reach**3 - min_reach**3) * w + min_reach**3) ** (1/3)
        x = r * np.sin(phi) * np.cos(theta)
        y = r * np.sin(phi) * np.sin(theta)
        z = r * np.cos(phi)
        point = base_position + np.array([x, y, z])
        # Nokta, base_position'a en az min_reach kadar uzakta mÄ±?
        if np.linalg.norm(point - base_position) >= min_reach:
            return point

def sample_random_quaternion():
    """
    Rastgele bir birim quaternion Ã¼retir.
    """
    axis = np.random.normal(0, 1, 3)
    axis /= np.linalg.norm(axis)
    angle = np.random.uniform(0, 2.0*np.pi)
    qw = np.cos(angle/2.0)
    qx = axis[0] * np.sin(angle/2.0)
    qy = axis[1] * np.sin(angle/2.0)
    qz = axis[2] * np.sin(angle/2.0)
    return np.array([qx, qy, qz, qw])

def create_orientation_marker(position, quaternion):
    """
    Tek bir marker (id=0) dÃ¶ndÃ¼rÃ¼r, bÃ¶ylece RViz'de her seferinde aynÄ± ID'ye
    sahip ok gÃ¼ncellenir ve eski okumalar silinmez, yalnÄ±zca gÃ¼ncellenir.
    """
    marker = Marker()
    marker.header.frame_id = 'world'
    marker.header.stamp = rclpy.time.Time().to_msg()
    marker.ns = 'target_orientation'
    marker.id = 0           # Sabit ID: 0. BÃ¶ylece tek bir ok gÃ¼ncellenir.
    marker.type = Marker.ARROW
    marker.action = Marker.ADD

    # Pozisyon ve oryantasyon
    marker.pose.position.x = float(position[0])
    marker.pose.position.y = float(position[1])
    marker.pose.position.z = float(position[2])
    marker.pose.orientation.x = float(quaternion[0])
    marker.pose.orientation.y = float(quaternion[1])
    marker.pose.orientation.z = float(quaternion[2])
    marker.pose.orientation.w = float(quaternion[3])

    # Ã–lÃ§eklendirme: ok kalÄ±nlÄ±ÄŸÄ± ve uzunluÄŸu
    marker.scale.x = 0.2   # ok uzunluÄŸu
    marker.scale.y = 0.05  # ok Ã§apÄ±
    marker.scale.z = 0.05  # ok Ã§apÄ±

    # Renk: yeÅŸil ok
    marker.color.r = 0.0
    marker.color.g = 1.0
    marker.color.b = 0.0
    marker.color.a = 1.0

    return marker

def main():
    rclpy.init()
    env = RobotEnv()
    executor = MultiThreadedExecutor()
    executor.add_node(env)
    executor_thread = threading.Thread(target=executor.spin, daemon=True)
    executor_thread.start()

    # RViz'e hem hedef pozisyonu hem de orientation marker yayÄ±nlanacak
    target_pub = env.create_publisher(PointStamped, 'target_position', 10)
    marker_pub = env.create_publisher(Marker, 'visualization_marker', 10)

    # Model ve replay buffer oluÅŸturma
    state_dim = 20
    action_dim = 6
    max_action = 0.5

    actor = Actor(state_dim, action_dim, max_action)
    actor_target = Actor(state_dim, action_dim, max_action)
    actor_target.load_state_dict(actor.state_dict())

    critic = Critic(state_dim, action_dim)
    critic_target = Critic(state_dim, action_dim)
    critic_target.load_state_dict(critic.state_dict())

    td3_agent = TD3(
        state_dim=state_dim,
        action_dim=action_dim,
        max_action=max_action,
        actor=actor,
        critic=critic,
        actor_target=actor_target,
        critic_target=critic_target,
        policy_freq=1
    )

    replay_buffer = ReplayBuffer()

    # Episode baÅŸÄ±na biriktirilecek listeler
    current_rewards = []
    current_actor_losses = []
    current_critic1_losses = []
    current_critic2_losses = []
    current_episodes = []

    previous_episodes = None
    previous_rewards = None
    previous_actor_losses = None
    previous_critic1_losses = None
    previous_critic2_losses = None

    # Checkpoint yÃ¼kleme
    load_choice = input("Checkpoint'ten yÃ¼klemek istiyor musunuz? (e/h): ").strip().lower()
    if load_choice == 'e':
        model_dirs = list_available_models()
        if model_dirs:
            print("Mevcut Modeller:")
            for model_dir in model_dirs:
                print(f"- {model_dir}")
            model_name = input("YÃ¼klemek istediÄŸiniz model adÄ±nÄ± girin: ").strip()
            success, (previous_episodes, previous_rewards) = load_checkpoint(td3_agent, replay_buffer, model_name)
            if not success:
                print("âš ï¸ Model yÃ¼klenemedi. SÄ±fÄ±rdan baÅŸlanacak.")
            else:
                # Ã–nceki loss dosyalarÄ±nÄ± da CSV'den yÃ¼klemeye Ã§alÄ±ÅŸ
                try:
                    import pandas as pd
                    model_dir = os.path.join("checkpoints", model_name)
                    actor_losses_path = os.path.join(model_dir, "actor_losses.csv")
                    critic1_losses_path = os.path.join(model_dir, "critic1_losses.csv")
                    critic2_losses_path = os.path.join(model_dir, "critic2_losses.csv")

                    if os.path.exists(actor_losses_path):
                        df_a = pd.read_csv(actor_losses_path)
                        previous_actor_losses = df_a['actor_loss'].values.tolist()
                    if os.path.exists(critic1_losses_path):
                        df_c1 = pd.read_csv(critic1_losses_path)
                        previous_critic1_losses = df_c1['critic1_loss'].values.tolist()
                    if os.path.exists(critic2_losses_path):
                        df_c2 = pd.read_csv(critic2_losses_path)
                        previous_critic2_losses = df_c2['critic2_loss'].values.tolist()
                except Exception as e:
                    print(f"âš ï¸ Ã–nceki loss dosyalarÄ± yÃ¼klenirken hata: {e}")
                    previous_actor_losses = None
                    previous_critic1_losses = None
                    previous_critic2_losses = None
        else:
            print("âš ï¸ KayÄ±tlÄ± model bulunamadÄ±. SÄ±fÄ±rdan baÅŸlanacak.")

    # GÃ¶rselleÅŸtiriciyi baÅŸlat
    visualizer = RewardVisualizer()

    episodes = 10000
    best_reward = float('inf')
    ACTION_TIMEOUT = 20

    # KeÅŸif gÃ¼rÃ¼ltÃ¼sÃ¼ ayarlarÄ±
    initial_sigma = 0.1 * max_action
    final_sigma   = 0.01 * max_action
    decay_steps   = 50000
    total_steps = 0

    try:
        for ep in range(episodes):
            # 1) Rastgele pozisyon ve oryantasyon Ã¶rnekle
            if ep < 1000:
                target_position = sample_target_ur5e(max_reach=0.2)
            elif ep < 1500:
                target_position = sample_target_ur5e(max_reach=0.4)
            elif ep < 2000:
                target_position = sample_target_ur5e(max_reach=0.6)
            else:
                target_position = sample_target_ur5e()  # default: 0.8

            print(f"Verilen hedef pozisyonu: {target_position}")
            target_quaternion = sample_random_quaternion()  # [qx, qy, qz, qw]

            # 2) RViz iÃ§in PointStamped
            point_msg = PointStamped()
            point_msg.header.frame_id = 'world'
            point_msg.header.stamp = env.get_clock().now().to_msg()
            point_msg.point.x, point_msg.point.y, point_msg.point.z = target_position

            # 3) RViz iÃ§in orientation Marker (id=0 sabit)
            marker = create_orientation_marker(target_position, target_quaternion)
            marker.header.stamp = env.get_clock().now().to_msg()

            print(f'\n--- Episode {ep+1} ---')

            # 4) Ortama reset: Pozisyon + Oryantasyon
            state = env.reset(target_position, target_quaternion.tolist())
            total_reward = 0.0
            done = False
            step_count = 0
            episode_start_time = time.time()  # Episode sayacÄ±nÄ± baÅŸlat

            # Episode boyunca biriktirilecek loss listeleri
            episode_actor_losses = []
            episode_critic1_losses = []
            episode_critic2_losses = []

            # 5) RViz'e hedef pozisyon ve markeri yayÄ±nla
            target_pub.publish(point_msg)
            marker_pub.publish(marker)

            while not done:
                iter_start = time.time()
                step_count += 1
                print(f'Ä°terasyon: {step_count}')

                # 6.1) TD3'den ham eylem al
                raw_action = td3_agent.select_action(np.array(state))

                # 6.2) Exploration noise ekle
                frac = min(1.0, total_steps / decay_steps)
                sigma = initial_sigma * (1 - frac) + final_sigma * frac
                noise = np.random.normal(0, sigma, size=action_dim)
                action = (raw_action + noise).clip(-max_action, max_action)

                # 6.3) Ortamda bir adÄ±m ilerle, Timeout kontrolÃ¼
                while True:
                    try:
                        next_state, reward, done = env.step(action)
                        break
                    except TimeoutError:
                        if time.time() - iter_start > ACTION_TIMEOUT:
                            print(f"\u23f0 {ACTION_TIMEOUT} saniyeyi aÅŸtÄ±. Episode iptal.")
                            env.teleport_to_home()
                            reward = -10.0
                            next_state = state
                            done = True
                            break
                        time.sleep(0.1)

                print(f'Reward: {reward:.2f}, Done: {done}')

                if done and reward == 0.0:
                    print("ï¸ Ã‡arpÄ±ÅŸma! Episode erken sonlandÄ±rÄ±ldÄ±.")
                    env.teleport_to_home()
                    break

                # 6.4) ReplayBuffer'a ekle
                replay_buffer.add(state, action, reward, next_state, float(done))

                # 6.5) Ã–ÄŸrenme ve loss hesaplama
                if replay_buffer.size() > 100:
                    actor_loss_val, critic1_loss_val, critic2_loss_val = td3_agent.train(replay_buffer)
                    print(f'Buffer size: {replay_buffer.size()}')

                    if actor_loss_val is not None:
                        episode_actor_losses.append(actor_loss_val)
                    episode_critic1_losses.append(critic1_loss_val)
                    episode_critic2_losses.append(critic2_loss_val)

                    print(f'  > Actor Loss: {actor_loss_val if actor_loss_val is not None else 0.0:.4f}, '
                          f'Critic1 Loss: {critic1_loss_val:.4f}, Critic2 Loss: {critic2_loss_val:.4f}')

                state = next_state
                total_reward += reward
                total_steps += 1

                if done:
                    print(f"ğŸ Episode sona erdi. Robot eve dÃ¶nÃ¼yor.")
                    env.teleport_to_home()
                    break

            # Episode bitti, sÃ¼reye baÄŸlÄ± Ã¶dÃ¼lÃ¼ hesapla ve ekle
            duration = time.time() - episode_start_time
            time_reward = 15 - duration  # 15 saniyeden hÄ±zlÄ±ysa Ã¶dÃ¼l, yavaÅŸsa ceza
            print(f'Episode sÃ¼resi: {duration:.2f}s, Zaman Ã¶dÃ¼lÃ¼: {time_reward:.2f}')
            total_reward += time_reward

            print(f'Episode {ep+1} Total Reward: {total_reward:.2f}')

            # 7) Episode Sonu: Ortalama loss hesaplama
            if len(episode_actor_losses) > 0:
                avg_actor_loss = float(np.mean(episode_actor_losses))
            else:
                avg_actor_loss = 0.0

            if len(episode_critic1_losses) > 0:
                avg_critic1_loss = float(np.mean(episode_critic1_losses))
            else:
                avg_critic1_loss = 0.0

            if len(episode_critic2_losses) > 0:
                avg_critic2_loss = float(np.mean(episode_critic2_losses))
            else:
                avg_critic2_loss = 0.0

            # Episode numarasÄ± (Ã¶nceki yÃ¼klemeden devam)
            if previous_episodes is not None and len(previous_episodes) > 0:
                current_episode_number = previous_episodes[-1] + len(current_episodes) + 1
            else:
                current_episode_number = len(current_episodes)
            current_episodes.append(current_episode_number)

            current_rewards.append(total_reward)
            current_actor_losses.append(avg_actor_loss)
            current_critic1_losses.append(avg_critic1_loss)
            current_critic2_losses.append(avg_critic2_loss)

            # 8) Her 500 epizotta otomatik checkpoint oluÅŸtur (farklÄ± bir dizin adÄ±yla)
            if (current_episode_number + 1) % 500 == 0:
                # Benzersiz bir isim iÃ§in zaman damgasÄ± ekleyelim
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                auto_name = f"auto_ep{current_episode_number+1}_{timestamp}"
                print(f"ğŸ”– Otomatik checkpoint kaydediliyor: {auto_name}")
                
                # Ã–nceki ve mevcut deÄŸerleri birleÅŸtir
                combined_episodes = []
                combined_actor_losses = []
                combined_critic1_losses = []
                combined_critic2_losses = []
                combined_rewards = []

                # Ã–nceki deÄŸerleri ekle (eÄŸer varsa)
                if previous_episodes is not None:
                    combined_episodes.extend(previous_episodes)
                    if previous_actor_losses is not None:
                        combined_actor_losses.extend(previous_actor_losses)
                    if previous_critic1_losses is not None:
                        combined_critic1_losses.extend(previous_critic1_losses)
                    if previous_critic2_losses is not None:
                        combined_critic2_losses.extend(previous_critic2_losses)
                    if previous_rewards is not None:
                        combined_rewards.extend(previous_rewards)

                # Mevcut deÄŸerleri ekle
                combined_episodes.extend(current_episodes)
                combined_actor_losses.extend(current_actor_losses)
                combined_critic1_losses.extend(current_critic1_losses)
                combined_critic2_losses.extend(current_critic2_losses)
                combined_rewards.extend(current_rewards)
                
                save_checkpoint(td3_agent, replay_buffer, combined_rewards, auto_name,
                              combined_episodes, combined_actor_losses,
                              combined_critic1_losses, combined_critic2_losses)

            # 10) Grafikleri GÃ¼ncelle
            visualizer.update(
                current_episodes, current_rewards,
                current_actor_losses, current_critic1_losses, current_critic2_losses,
                previous_episodes, previous_rewards,
                previous_actor_losses, previous_critic1_losses, previous_critic2_losses
            )
            print(f" GÃ¶rselleÅŸtirme gÃ¼ncellendi. Toplam {len(current_rewards)} episode.")

    except KeyboardInterrupt:
        print("\n EÄŸitim kullanÄ±cÄ± tarafÄ±ndan durduruldu.")
        save_choice = input("Checkpoint ve deÄŸerler kaydedilsin mi? (e/h): ").strip().lower()
        if save_choice == 'e':
            model_name = input("Kaydetmek iÃ§in model adÄ± girin: ").strip()
            
            # Ã–nceki ve mevcut deÄŸerleri birleÅŸtir
            combined_episodes = []
            combined_actor_losses = []
            combined_critic1_losses = []
            combined_critic2_losses = []
            combined_rewards = []

            # Ã–nceki deÄŸerleri ekle (eÄŸer varsa)
            if previous_episodes is not None:
                combined_episodes.extend(previous_episodes)
                if previous_actor_losses is not None:
                    combined_actor_losses.extend(previous_actor_losses)
                if previous_critic1_losses is not None:
                    combined_critic1_losses.extend(previous_critic1_losses)
                if previous_critic2_losses is not None:
                    combined_critic2_losses.extend(previous_critic2_losses)
                if previous_rewards is not None:
                    combined_rewards.extend(previous_rewards)

            # Mevcut deÄŸerleri ekle
            combined_episodes.extend(current_episodes)
            combined_actor_losses.extend(current_actor_losses)
            combined_critic1_losses.extend(current_critic1_losses)
            combined_critic2_losses.extend(current_critic2_losses)
            combined_rewards.extend(current_rewards)
            
            save_checkpoint(td3_agent, replay_buffer, combined_rewards, model_name,
                          combined_episodes, combined_actor_losses,
                          combined_critic1_losses, combined_critic2_losses)
        else:
            print(" Kaydetmeden Ã§Ä±kÄ±lÄ±yor.")
    finally:
        visualizer.close()
        env.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
